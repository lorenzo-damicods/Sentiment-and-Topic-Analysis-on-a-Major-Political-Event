ğŸ“˜ # 1 â€” Introduction

This notebook performs sentiment analysis (BERT) and topic modeling (LDA) on news articles related to the 2024 attempted attack on Donald Trump.

The dataset is not stored in this repository due to licensing constraints.
To generate it locally, run:

export NEWSAPI_KEY="your_key"
python newsAPI_dataset_generator.py
python GDELT_API_dataset_generator.py


All generated CSV files will appear inside the dataset/ folder.

ğŸ’» Cell 1 â€” Load Dataset
import os
import pandas as pd

DATA_PATH = os.path.join("dataset", "combined_trump_data_cleaned.csv")

if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(
        f"Dataset not found at: {DATA_PATH}\n\n"
        "âš ï¸ The dataset is NOT included in the repository due to licensing and size constraints.\n"
        "Generate it by running:\n"
        "   python newsAPI_dataset_generator.py\n"
        "   python GDELT_API_dataset_generator.py\n"
        "Then restart this notebook."
    )

df = pd.read_csv(DATA_PATH)
df.head()

ğŸ“˜ # 2 â€” Data Cleaning & Preprocessing

We apply:

lowercase normalization

punctuation removal

stopword filtering

tokenization

lemmatization

These steps ensure that both sentiment analysis and topic modeling operate on clean, normalized text.

ğŸ’» Cell 2 â€” Ensure NLTK resources
import nltk

def ensure_nltk(path, name):
    try:
        nltk.data.find(path)
    except LookupError:
        nltk.download(name, quiet=True)

ensure_nltk("corpora/stopwords", "stopwords")
ensure_nltk("corpora/wordnet", "wordnet")
ensure_nltk("tokenizers/punkt", "punkt")

ğŸ’» Cell 3 â€” Text cleaning functions
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"<.*?>", "", text)             # remove HTML
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    return " ".join(tokens)

df["cleaned_content"] = df["content"].astype(str).apply(clean_text)
df.head()

ğŸ“˜ # 3 â€” Sentiment Analysis (BERT)

We use a pretrained BERT model (transformers library) to classify text sentiment as:

positive

neutral

negative

This gives an emotional profile of media coverage.

ğŸ’» Cell 4 â€” Sentiment model
from transformers import pipeline
import logging

logging.getLogger("transformers").setLevel(logging.ERROR)

sentiment_model = pipeline("sentiment-analysis")

ğŸ’» Cell 5 â€” Apply BERT to dataset
df["sentiment"] = df["cleaned_content"].apply(
    lambda x: sentiment_model(x[:512])[0]["label"] if isinstance(x, str) else "UNKNOWN"
)
df["sentiment"].value_counts()

ğŸ“˜ # 4 â€” Sentiment Visualization
ğŸ’» Cell 6 â€” Plot sentiment distribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.countplot(data=df, x="sentiment", palette="viridis")
plt.title("Sentiment Distribution (BERT)")
plt.show()

ğŸ“˜ # 5 â€” Topic Modeling (LDA)

We use Gensim LDA to extract the main themes across articles.
Each document is assigned a dominant topic.

ğŸ’» Cell 7 â€” Prepare text for LDA
from gensim import corpora
from gensim.models import LdaModel

texts = df["cleaned_content"].apply(lambda x: x.split())
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(t) for t in texts]

ğŸ’» Cell 8 â€” Train LDA model
lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=5,
    passes=10,
    random_state=42
)

lda_model.print_topics()

ğŸ’» Cell 9 â€” Dominant topic per document
def get_dominant_topic(model, bow):
    topics = model[bow]
    topics = sorted(topics, key=lambda x: x[1], reverse=True)
    return topics[0][0] if topics else None

df["dominant_topic"] = [get_dominant_topic(lda_model, bow) for bow in corpus]
df["dominant_topic"].value_counts()

ğŸ“˜ # 6 â€” Visualization: Topic Distribution
ğŸ’» Cell 10 â€” Plot dominant topic counts
plt.figure(figsize=(8,5))
sns.countplot(data=df, x="dominant_topic", palette="magma")
plt.title("Document Count per Dominant Topic")
plt.show()

ğŸ“˜ # 7 â€” Word Frequency (Optional)

A simple word frequency analysis highlights key recurring terms.

ğŸ’» Cell 11 â€” Word frequency
from collections import Counter

all_words = " ".join(df["cleaned_content"]).split()
word_freq = Counter(all_words).most_common(20)
word_freq

ğŸ“˜ # 8 â€” Conclusions

Coverage shows predominantly negative sentiment, consistent with the severe nature of the event.

LDA topic modeling reveals themes about:
politics, security, public reaction, media framing, investigation.

Combining sentiment + topics helps identify bias patterns and media positioning.

ğŸ“˜ End of notebook
